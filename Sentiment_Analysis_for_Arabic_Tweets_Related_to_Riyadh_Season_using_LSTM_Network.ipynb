{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Sentiment Analysis for Arabic Tweets Related to Riyadh Season using LSTM Network",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn3fIa_EcKdG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, SimpleRNN, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csvFile = open('Tweets.csv', 'a')\n",
        "csvWriter = csv.writer(csvFile)\n"
      ],
      "metadata": {
        "id": "tIE5EN8RdSgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in tw.Cursor(api.search,\n",
        "                           q=HashValue,\n",
        "                           count=20,\n",
        "                           lang=\"ar\",\n",
        "                           since=date_since,\n",
        "                           tweet_mode='extended').items(300):\n",
        "    \n",
        "    print (tweet.created_at, tweet.full_text)\n",
        "    csvWriter.writerow([tweet.created_at, tweet.full_text.encode('utf-8-sig')])\n",
        "\n",
        "print (\"Scraping finished and saved to \"+HashValue+\".csv\")"
      ],
      "metadata": {
        "id": "BGFU8o0gdWh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collecting data\n",
        "\n",
        "tweets = tw.Cursor(api.search,q=HashValue,lang=\"ar\", since=date_since).items(300)\n",
        "\n",
        "users_locs = [[tweet.text,tweet.user.screen_name, tweet.user.location] for tweet in tweets]\n",
        "\n",
        "tweet_text = pd.DataFrame(data=users_locs,  columns=['tweet','user', \"location\"])\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "\n",
        "tweet_text.to_csv('1.csv')\n",
        "\n",
        "tweet_text #unlabeld dataset\n"
      ],
      "metadata": {
        "id": "KAIy7uJ1dXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labeld dataset \n",
        "data = pd.read_csv('Riyadh_Season.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['tweet','Category']]\n",
        "data\n"
      ],
      "metadata": {
        "id": "Ld-bzAy9eFrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning data\n",
        "\n",
        "def clean_text(text):  \n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
        "              \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
        "               \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
        "    \n",
        "    tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(tashkeel,\"\", text)  # remove Shadda,Fatha,Tanwin,Kasra..\n",
        "    \n",
        "    longation = re.compile(r'(.)\\1+') \n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(longation, subst, text)   #remove longation  [إأآا]\", \"ا\" or \"ة\", \"ه\",  ...\n",
        "    \n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "    text = re.sub(r\"[a-zA-Z]\", '', text)\n",
        "    text = re.sub(r\"\\d+\", ' ', text)\n",
        "    text = re.sub(r\"\\n+\", ' ', text)\n",
        "    text = re.sub(r\"\\t+\", ' ', text)\n",
        "    text = re.sub(r\"\\r+\", ' ', text)\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    #text = data.strip()\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "b_MUDU12h9Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data['cleaned_text'] = data.tweet.apply(clean_text)\n",
        "\n",
        "data = data[data.cleaned_text != \"\"] #reomve any empty fields ?\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "G4W1rUVdlvpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num_samples in each category (Positive 1,negative -1,nautral 0)\n",
        "#its a unbalanced dataset\n",
        "data.groupby(['Category']).count()"
      ],
      "metadata": {
        "id": "rjwU15YPovr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dealing with the unbalance \n",
        "\n",
        "min_sample = data.groupby(['Category']).count().cleaned_text.min()\n",
        "b_data = pd.concat([data[data.Category == 1].head(min_sample), \n",
        "                        data[data.Category == -1].head(min_sample),\n",
        "                     data[data.Category == 0].head(min_sample)])\n",
        "\n",
        "b_data.groupby(['Category']).count()"
      ],
      "metadata": {
        "id": "4e-2CC6T6Lro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.cleaned_text.values #input\n",
        "\n",
        "Y = data.Category.values.astype('float32') #target\n"
      ],
      "metadata": {
        "id": "DhDrKBIbpaVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization Process\n",
        "\n",
        "maxlen = 300\n",
        "max_fatures = 800\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['cleaned_text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['cleaned_text'].values)\n",
        "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
        "X\n"
      ],
      "metadata": {
        "id": "9EA3_Phvlavg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=20"
      ],
      "metadata": {
        "id": "2kmFntAS8HKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=seed)\n",
        "\n",
        "print(\"Training:\", len(X_train), len(Y_train))\n",
        "print(\"Testing: \", len(X_test), len(Y_test))"
      ],
      "metadata": {
        "id": "wjfkArPO72B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.backend import clear_session\n",
        "\n",
        "embedding_dim = 200\n",
        "dropout = 0.5\n",
        "opt = 'adam'\n",
        "clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=max_fatures, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Bidirectional(layers.LSTM(50, dropout=dropout, \n",
        "                                           recurrent_dropout=dropout, \n",
        "                                           return_sequences=True)))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(dropout))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dropout(dropout))\n",
        "model.add(layers.Dense(1, activation='relu'))\n",
        "\n",
        "model.compile(optimizer=opt, \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1I8YM4JE8TYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    batch_size=64)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, Y_train, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "#TESTING ?\n",
        "loss_val, accuracy_val = model.evaluate(X_test, Y_test, verbose=True)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))"
      ],
      "metadata": {
        "id": "ZhVAAfnwvcrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compre results:\n",
        "\n",
        "df_blind = pd.DataFrame({'REAL': Y_blind, \n",
        "                         'PRED': pred_blind.reshape(pred_blind.shape[0],), \n",
        "                         'TEXT': blind_test.cleaned_text})\n",
        "df_blind = df_blind.reset_index()[['REAL', 'PRED', 'TEXT']]\n",
        "df_blind.PRED = df_blind.PRED.round()\n",
        "error_records = df_blind[df_blind.REAL != df_blind.PRED]\n",
        "print(\"Number of misclassified reviews: {} out of {}\".format(error_records.shape[0], df_blind.shape[0]))\n",
        "print(\"Blind Test Accuracy:  {:.4f}\".format(accuracy_score(df_blind.REAL, df_blind.PRED)))"
      ],
      "metadata": {
        "id": "UYqZ-r_7v632"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample outputs:\n",
        "\n",
        "df_blind.sample(n=3)"
      ],
      "metadata": {
        "id": "kTrXBFWuv_jY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}